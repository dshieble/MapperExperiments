{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "PROJECTION = \"knn_distance_2\"\n",
    "NUM_RUNS = 100\n",
    "DATASET = \"WORD_VECTOR\"\n",
    "SAMPLE_STEP = 50\n",
    "MAX_SAMPLES = 500\n",
    "WITH_REPLACEMENT = True\n",
    "N_JOBS = 1\n",
    "N_CUBES = 10\n",
    "P_OVERLAP = 0.5\n",
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_distance_2_100_WORD_VECTOR_50_500_True_10_0.5_3\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NAME = \"{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "    PROJECTION, NUM_RUNS, DATASET, SAMPLE_STEP, MAX_SAMPLES, WITH_REPLACEMENT, N_CUBES, P_OVERLAP, N_CLUSTERS)\n",
    "print(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.estrada_index.html#networkx.algorithms.centrality.estrada_index\n",
    "https://kepler-mapper.scikit-tda.org/notebooks/Adapters.html\n",
    "\"\"\"\n",
    "\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn\n",
    "# !pip install networkx\n",
    "# !pip install umap\n",
    "# !pip install umap-learn\n",
    "# !pip install kmapper\n",
    "# !pip install gensim\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import spearmanr\n",
    "from datetime import date, datetime\n",
    "import calendar\n",
    "from scipy.optimize import minimize_scalar\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from umap.umap_ import UMAP\n",
    "import networkx as nx\n",
    "import os\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from networkx.algorithms.approximation.treewidth import treewidth_min_fill_in\n",
    "import kmapper as km\n",
    "import os\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from networkx.algorithms.approximation.treewidth import treewidth_min_fill_in\n",
    "from networkx.algorithms.centrality import estrada_index\n",
    "from networkx.algorithms.cycles import cycle_basis \n",
    "import gensim.downloader as api\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 20\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def get_mnist_dataset(skip=1):\n",
    "    mnist_path = 'fashion'\n",
    "\n",
    "    raw_Xtrain, raw_ytrain = load_mnist(mnist_path, kind='train')\n",
    "    raw_Xtest, raw_ytest = load_mnist(mnist_path, kind='t10k')\n",
    "\n",
    "    raw_Xtrain = raw_Xtrain / 255\n",
    "    raw_Xtest = raw_Xtest / 255\n",
    "\n",
    "    train_indices = np.random.permutation(range(raw_Xtrain.shape[0]))[::skip]\n",
    "    test_indices = np.random.permutation(range(raw_Xtest.shape[0]))[::skip]\n",
    "\n",
    "    raw_Xtrain = raw_Xtrain[train_indices, :]\n",
    "    raw_ytrain = raw_ytrain[train_indices]\n",
    "\n",
    "    raw_Xtest = raw_Xtest[test_indices, :]\n",
    "    raw_ytest = raw_ytest[test_indices]\n",
    "    return raw_Xtrain, raw_ytrain, raw_Xtest, raw_ytest\n",
    "\n",
    "def get_word_vector_dataset():\n",
    "\n",
    "    model = api.load(\"glove-wiki-gigaword-50\")\n",
    "    vocab = list(model.vocab.keys())\n",
    "    embeddings = np.array([model.get_vector(w) for w in vocab])\n",
    "\n",
    "    D = {\"vocab\": vocab, \"embeddings\": embeddings}\n",
    "    return D[\"embeddings\"], np.array(D[\"vocab\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_mapper_graph(data, labels, title, visualize, projection, verbose=0):\n",
    "    path_html = \"/Users/dshiebler/workspace/data/mapper/{}.html\".format(title)\n",
    "\n",
    "    # Initialize\n",
    "    mapper = km.KeplerMapper(verbose=verbose)\n",
    "\n",
    "    # Fit to and transform the data\n",
    "#     projected_data = mapper.fit_transform(data, projection=UMAP(n_components=2))\n",
    "#     projected_data = mapper.fit_transform(data, projection=PCA(n_components=1))\n",
    "\n",
    "    if projection == \"pca_{}\":\n",
    "        n_components = int(projection.split(\"_\")[-1])\n",
    "        projection = PCA(n_components=n_components)\n",
    "    elif projection == \"umap_1\":\n",
    "        n_components = int(projection.split(\"_\")[-1])\n",
    "        projection = UMAP(n_components=n_components)\n",
    "        \n",
    "    projected_data = mapper.fit_transform(data, projection=projection)\n",
    "\n",
    "    # Create dictionary called 'graph' with nodes, edges and meta-information\n",
    "    cover = km.Cover(n_cubes=N_CUBES, perc_overlap=P_OVERLAP)\n",
    "    graph = mapper.map(projected_data, data, cover=cover, clusterer=AgglomerativeClustering(N_CLUSTERS))\n",
    "\n",
    "    # Visualize it\n",
    "    if visualize:\n",
    "        path_html = mapper.visualize(\n",
    "          graph, path_html=path_html,\n",
    "          title=title, custom_tooltips=np.array([str(l) for l in labels]))\n",
    "        out = graph, path_html\n",
    "    else:\n",
    "        out = graph\n",
    "    return out\n",
    "\n",
    "\n",
    "def nodes_in_component(component, graph):\n",
    "    return sum([len(graph[\"nodes\"][cluster]) for cluster in component])\n",
    "\n",
    "def connected_component_node_counts(graph, nx_graph):\n",
    "    cc_clusters = [cc for cc in nx.connected_components(nx_graph)]\n",
    "    return [nodes_in_component(cc, graph) for cc in cc_clusters]\n",
    "\n",
    "def resample(X, n_samples, with_replacement):\n",
    "    indices = np.arange(0, X.shape[0])\n",
    "    return X[np.random.choice(indices, size=n_samples, replace=with_replacement)]\n",
    "\n",
    "\n",
    "def get_metrics(n_samples):\n",
    "    return {\n",
    "        \"num_cc_p0\": lambda graph, nx_graph: len([cc for cc in connected_component_node_counts(graph, nx_graph)]),\n",
    "        \"density\": lambda graph, nx_graph: nx.density(nx_graph),\n",
    "        \"estrada_index\": lambda graph, nx_graph: estrada_index(nx_graph),\n",
    "        # \"treewidth\": lambda graph: treewidth_min_fill_in(km.adapter.to_nx(graph))[0],\n",
    "        \"num_basis_cycles\": lambda graph, nx_graph: len(cycle_basis(nx_graph))\n",
    "    }\n",
    "\n",
    "\n",
    "def get_metric_values(n_samples):\n",
    "    if N_JOBS == 1:\n",
    "        print(\"calling get_graph with n_samples={}\".format(n_samples))\n",
    "\n",
    "    metric_to_sample_values = {metric_name: {n_samples: []} for metric_name in get_metrics(1).keys()}\n",
    "\n",
    "    iterator = tqdm(range(NUM_RUNS)) if N_JOBS == 1 else range(NUM_RUNS)\n",
    "    for i in iterator:\n",
    "        data = resample(X, n_samples, with_replacement=WITH_REPLACEMENT)\n",
    "        graph = build_mapper_graph(\n",
    "            data, labels=Y, projection=PROJECTION, title=\"my_html-{}\".format(i), visualize=False, verbose=0)\n",
    "        nx_graph = km.adapter.to_nx(graph)\n",
    "        for metric_name, metric_fn in get_metrics(n_samples).items():\n",
    "            metric_to_sample_values[metric_name][n_samples].append(metric_fn(graph, nx_graph))\n",
    "\n",
    "    return metric_to_sample_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_VECTOR X.shape: (400000, 50) Y.shape: (400000,)\n"
     ]
    }
   ],
   "source": [
    "if \"MNIST_UMAP_\" in DATASET:\n",
    "    n_components = int(DATASET.split(\"_\")[-1])\n",
    "    rawX, Y, _, _ = get_mnist_dataset(skip=1)\n",
    "    X = UMAP(n_components=n_components, n_neighbors=15).fit_transform(rawX, y=Y)\n",
    "elif DATASET == \"WORD_VECTOR\":\n",
    "    X, Y = get_word_vector_dataset()\n",
    "else:\n",
    "    raise ValueError(\"Dataset not recognized\")\n",
    "\n",
    "print(\"{} X.shape: {} Y.shape: {}\".format(DATASET, X.shape, Y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:00<00:00, 136.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric_to_sample_values_list with N_JOBS=1...\n",
      "calling get_graph with n_samples=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 30/100 [00:00<00:00, 142.34it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:00<00:00, 147.62it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:00<00:00, 150.30it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:00<00:00, 150.87it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 152.79it/s][A\n",
      " 11%|█         | 1/9 [00:00<00:05,  1.52it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:00<00:00, 124.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 27/100 [00:00<00:00, 127.54it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:00<00:00, 126.97it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:00<00:00, 128.90it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:00<00:00, 121.55it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:00<00:00, 109.20it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 117.17it/s][A\n",
      " 22%|██▏       | 2/9 [00:01<00:05,  1.39it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:00<00:00, 102.82it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 21/100 [00:00<00:00, 101.29it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:00<00:00, 101.85it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:00<00:00, 103.14it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:00<00:00, 103.89it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:00<00:00, 100.95it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:00<00:00, 101.64it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:00<00:00, 106.00it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.93it/s][A\n",
      " 33%|███▎      | 3/9 [00:02<00:04,  1.27it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:00<00:00, 100.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 20/100 [00:00<00:00, 93.91it/s] \u001b[A\n",
      " 29%|██▉       | 29/100 [00:00<00:00, 92.11it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:00<00:00, 91.00it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:00<00:00, 91.72it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:00<00:00, 90.46it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:00<00:00, 89.72it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:00<00:00, 89.88it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:00<00:00, 89.69it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:01<00:00, 89.62it/s][A\n",
      " 44%|████▍     | 4/9 [00:03<00:04,  1.13it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:00<00:01, 71.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 15/100 [00:00<00:01, 70.94it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:00<00:01, 70.58it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:00<00:00, 70.94it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:00<00:00, 71.32it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:00<00:00, 72.04it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:00<00:00, 74.48it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:00<00:00, 72.97it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:00<00:00, 74.38it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:01<00:00, 75.20it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:01<00:00, 75.50it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:01<00:00, 73.44it/s][A\n",
      " 56%|█████▌    | 5/9 [00:04<00:04,  1.03s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:00<00:01, 62.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 14/100 [00:00<00:01, 61.82it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:00<00:01, 61.77it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:00<00:01, 61.90it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:00<00:01, 62.15it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:00<00:00, 62.73it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:00<00:00, 62.75it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:00<00:00, 63.21it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:01<00:00, 62.70it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:01<00:00, 62.61it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:01<00:00, 62.54it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:01<00:00, 62.53it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:01<00:00, 63.23it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.03it/s][A\n",
      " 67%|██████▋   | 6/9 [00:06<00:03,  1.20s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:00<00:01, 55.54it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 12/100 [00:00<00:01, 55.34it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:00<00:01, 54.73it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:00<00:01, 54.36it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:00<00:01, 53.89it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:00<00:01, 54.25it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:00<00:01, 54.40it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:00<00:00, 54.48it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:00<00:00, 54.30it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:01<00:00, 54.28it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:01<00:00, 54.56it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:01<00:00, 54.66it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:01<00:00, 55.04it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:01<00:00, 54.93it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:01<00:00, 54.13it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.28it/s][A\n",
      " 78%|███████▊  | 7/9 [00:08<00:02,  1.39s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:00<00:02, 47.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 10/100 [00:00<00:01, 47.35it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:00<00:01, 47.35it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:00<00:01, 47.19it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:00<00:01, 46.95it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:00<00:01, 46.05it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:00<00:01, 46.24it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:00<00:01, 46.30it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:00<00:01, 46.52it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:01<00:01, 46.18it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:01<00:01, 44.96it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:01<00:00, 45.05it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:01<00:00, 45.08it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:01<00:00, 45.39it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:01<00:00, 45.76it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:01<00:00, 45.98it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:01<00:00, 45.41it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:01<00:00, 46.02it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:02<00:00, 45.73it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:02<00:00, 45.91it/s]\u001b[A\n",
      " 89%|████████▉ | 8/9 [00:10<00:01,  1.63s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:00<00:02, 40.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_graph with n_samples=450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▉         | 9/100 [00:00<00:02, 38.37it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:00<00:02, 38.56it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:00<00:02, 38.48it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:00<00:02, 38.61it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:00<00:01, 38.15it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:00<00:01, 38.51it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:00<00:01, 38.23it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:00<00:01, 38.69it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:01<00:01, 39.17it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:01<00:01, 39.10it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:01<00:01, 39.69it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:01<00:01, 40.29it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:01<00:00, 40.27it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:01<00:00, 40.57it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:01<00:00, 40.70it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:01<00:00, 40.92it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:02<00:00, 40.91it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:02<00:00, 41.00it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:02<00:00, 41.24it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:02<00:00, 40.00it/s][A\n",
      "100%|██████████| 9/9 [00:13<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_to_sample_values_list computed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from parallel_process import parallel_process\n",
    "\n",
    "\n",
    "print(\"Computing metric_to_sample_values_list with N_JOBS={}...\".format(N_JOBS))\n",
    "if N_JOBS == 1:\n",
    "    metric_to_sample_values_list = []\n",
    "    for n_samples in tqdm(range(SAMPLE_STEP, MAX_SAMPLES, SAMPLE_STEP)):\n",
    "        metric_to_sample_values_list.append(get_metric_values(n_samples))\n",
    "else:\n",
    "    metric_to_sample_values_list = parallel_process(\n",
    "        array=range(SAMPLE_STEP, MAX_SAMPLES, SAMPLE_STEP),\n",
    "        function=get_metric_values,\n",
    "        n_jobs=N_JOBS)\n",
    "print(\"metric_to_sample_values_list computed!\")\n",
    "\n",
    "\n",
    "out = {}\n",
    "out[\"metric_to_sample_values\"] = {metric_name: {} for metric_name in get_metrics(1).keys()}\n",
    "for metric_to_sample_values in metric_to_sample_values_list:\n",
    "    for metric in out[\"metric_to_sample_values\"]:\n",
    "        out[\"metric_to_sample_values\"][metric].update(metric_to_sample_values[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "out[\"metric_to_sample_values\"] = {metric_name: {} for metric_name in get_metrics(1).keys()}\n",
    "for metric_to_sample_values in metric_to_sample_values_list:\n",
    "    for metric in out[\"metric_to_sample_values\"]:\n",
    "        out[\"metric_to_sample_values\"][metric].update(metric_to_sample_values[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
